---
title: "exercise 2"
author: "Ryan Clorfeine"
date: "2/26/2020"
output: 
  md_document:
    variant: gfm
---
# Exercise 2
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(mosaic)
library(FNN)
library(foreach)
library(scales)
library(knitr)
```

## KNN Practice

**Goal:** Build two KNN models for 350 and 65 AMG trim Mercedes to predict car price given a car's mileage.

***

First we split the data based on whether the car has a trim level of 350 or 65 AMG. In the plot below we compare price vs mileage for each trim.

```{r echo=FALSE}
sclass = read.csv("sclass.csv")

# Split data based on trim
trim_350 = sclass[sclass$trim == "350",]
trim_65 = sclass[sclass$trim == "65 AMG",]

ggplot(subset(sclass, trim == "350" | trim =="65 AMG")) +
  geom_point(aes(mileage, price))+
  facet_wrap(~trim)
```

As we can see from the above scatterplot, there is a group of 65 AMG cars with 0 mileage which have a significantly higher price than the 350 trim cars. This will affect how the two models are built later.

Next we need to  split the data into training and test tests and use the training sets to fit two KNN models. This leads to the question of how to pick the model parameters K for each model. One way to do this is to pick the K which gives the minimum root mean square error (rmse) using the test sets. Below is a plot of RMSE vs K for each model:

```{r echo=FALSE}
# Define the training and testing set for 350 trim
N_350 = nrow(trim_350)
N_train_350 = floor(0.8*N_350)
N_test_350 = N_350 - N_train_350
train_ind_350 = sample.int(N_350, N_train_350, replace=FALSE)
D_train_350 = trim_350[train_ind_350,]
D_test_350 = trim_350[-train_ind_350,]
D_test_350 = arrange(D_test_350, mileage)
x_train_350 = select(D_train_350,mileage)
y_train_350 = select(D_train_350,price)
x_test_350 = select(D_test_350, mileage)
y_test_350 = select(D_test_350, price)

# Define the training and testing set for 65 AMG trim
N_65 = nrow(trim_65)
N_train_65= floor(0.8*N_65)
N_test_65 = N_65 - N_train_65
train_ind_65 = sample.int(N_65, N_train_65, replace=FALSE)
D_train_65 = trim_65[train_ind_65,]
D_test_65 = trim_65[-train_ind_65,]
D_test_65 = arrange(D_test_65, mileage)
x_train_65 = select(D_train_65,mileage)
y_train_65 = select(D_train_65,price)
x_test_65 = select(D_test_65, mileage)
y_test_65 = select(D_test_65, price)
```

```{r echo=FALSE}

rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
fit_KNN_model <- function(k, x_train, x_test, y_train, y_test){
  knnModel = knn.reg(train=x_train, test=x_test, y = y_train, k=k)
  rmse(y_test, knnModel$pred)
}

k_grid_350 = 3:250
rmse_grid_350 = foreach(K=k_grid_350, .combine='c') %do% {
  fit_KNN_model(K,x_train_350, x_test_350, y_train_350, y_test_350)
}

k_grid65 = 3:200
rmse_grid_65 = foreach(K=k_grid65, .combine='c') %do% {
  
  fit_KNN_model(K,x_train_65, x_test_65, y_train_65,y_test_65)
}

```

```{r comment=NA, echo=FALSE}
ggplot()+
  geom_point(data.frame(K=k_grid_350,RMSE=rmse_grid_350),mapping=aes(K,RMSE)) +
  ggtitle(("RMSE vs K for 350 Trim"))

minK_350 = data.frame(x=k_grid_350,y=rmse_grid_350) %>% subset(y == min(rmse_grid_350))
cat("The KNN model with the minimum RMSE is fitted with K =",minK_350[1,1], "for a trim level of 350." )

ggplot()+
  geom_point(data.frame(K=k_grid65,RMSE=rmse_grid_65),mapping=aes(K,RMSE)) +
  ggtitle(("RMSE vs K for 65 AMG Trim"))

minK_65 = data.frame(x=k_grid65,y=rmse_grid_65) %>% subset(y == min(rmse_grid_65))
cat("The KNN model with the minimum RMSE is fitted with K =",minK_65[1,1], "for a trim level of 65 AMG.")
```

Now that we have fitted each model with a K parameter which minimizes RMSE, we can plot both models over its corresponding test data to visualize how the predictions (in red) compared with the actual values (grey points).

```{r echo=FALSE}
# fit knn models for optimal k
knnModel350 = knn.reg(train=x_train_350, test=x_test_350, y = y_train_350, k=minK_350[1,1])
knnModel65 = knn.reg(train=x_train_65, test=x_test_65, y = y_train_65, k=minK_65[1,1])

# attach the predictions to the test data frame
D_test_350$ypred_knn = knnModel350$pred
D_test_65$ypred_knn = knnModel65$pred

ggplot(data=D_test_350) + 
  geom_point(mapping = aes(x=mileage, price), color = "lightgrey") +
  geom_path(mapping = aes(x=mileage, y= ypred_knn), color="red") +
  theme_bw()+
  ggtitle("KNN Model for Price vs Mileage for 350M Trim")+
  scale_x_continuous(labels = scales::comma)

ggplot(data=D_test_65) + 
  geom_point(mapping = aes(x=mileage, price),color = "lightgrey") +
  geom_path(mapping = aes(x=mileage, y= ypred_knn), color="red" ) +
  theme_bw()+
  ggtitle("KNN Model for Price vs Mileage for 65 AMG Trim")+
  scale_x_continuous(labels = scales::comma)
 
```

The 65 AMG model is always fitted with a higher K value than the 350 trim model. At the beginning of the analysis, it was pointed out that there is a group of outlier  65 AMG trim cars which have significantly higher price and 0 mileage. If we chose a lower K value for the 65 AMG trim model, our predictions would have higher variance because the model would memorize the noise of the outliers. Since the 350 trim model has less extreme outliers choosing a smaller K doesn't lead to as much error as choosing a small K would in the 65 AMG model. Therefore a larger K minimizes RMSE for the 65 AMG model and the oppositie is true for the 350 trim model.

## Saratoga House Prices

**Goal: Build Linear and KNN models which predict Saratoga House Prices**

***

```{r echo=FALSE, warning=FALSE}
data(SaratogaHouses)
SaratogaHouses$roomSize = SaratogaHouses$livingArea / SaratogaHouses$rooms
n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train

rmse_vals = do(100)*{
  
  # split into train and test cases with the same sample sizes
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  
  #fit medium model to training data
  lm_medium = lm(price ~ lotSize + age + livingArea + pctCollege + bedrooms + 
                 fireplaces + rooms + heating + fuel + centralAir, data=saratoga_train)
  #fit new model to beat medium model
  lm_new = lm(price ~ lotSize * livingArea + age + bedrooms + lotSize:bathrooms  + roomSize  + waterfront + centralAir + newConstruction + landValue:lotSize + livingArea:waterfront +fireplaces:lotSize, data=saratoga_train)
  
  yhat_test_medium = predict(lm_medium, saratoga_test)
  yhat_test_new = predict(lm_new, saratoga_test)
  rmse(saratoga_test$price, yhat_test_medium)
  c(rmse(saratoga_test$price, yhat_test_medium), rmse(saratoga_test$price, yhat_test_new))
}

kable(data.frame(Medium_Model = colMeans(rmse_vals)[1], New_Model = colMeans(rmse_vals)[2]), row.names = FALSE, format="markdown", align = "c", caption = "Average RMSE for  in-class Medium Model vs New Model")

```

The above table shows the root mean square error for the medium model created in class vs the RMSE for my model. Compared to the medium model, the new model has a lower RMSE.The new model uses the equation:

 price = lotSize * livingArea + age + bedrooms + lotSize:bathrooms  + roomSize  + waterfront + centralAir + newConstruction + landValue:lotSize + livingArea:waterfront + fireplaces:lotSize


```{r echo=FALSE}
coef(lm_new)
```


